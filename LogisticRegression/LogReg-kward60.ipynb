{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Binary Cross Entropy Loss with Gradient Descent\n",
    "\n",
    "### Kyle Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to system path to import mlutils\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import mlutils\n",
    "\n",
    "# Global vars\n",
    "# File headers\n",
    "headers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes\n",
    "#### *I realize the following two helper class should've just been global methods but its too late now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser for raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    # Read in dataset (code from logistic-regression.ipynb)\n",
    "    @staticmethod\n",
    "    def read_data(filename):\n",
    "        global headers \n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        p = re.compile(',')\n",
    "        xdata = []\n",
    "        ydata = []\n",
    "        header = f.readline().strip()\n",
    "        headers.append(header)\n",
    "        varnames = p.split(header)\n",
    "        namehash = {}\n",
    "        for l in f:\n",
    "            li = p.split(l.strip())\n",
    "            xdata.append([float(x) for x in li[:-1]])\n",
    "            ydata.append(float(li[-1]))\n",
    "        \n",
    "        return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model visualizer for plotting training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Visualizer class\n",
    "class ModelVisualizer:\n",
    "\n",
    "    # Plot losses over iterations\n",
    "    @staticmethod\n",
    "    def plot_loss(losses, xlabel='', ylabel='', title=''):\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(7,8))\n",
    "\n",
    "        # Create a linear space to plot\n",
    "        x_axis = np.linspace(0,len(losses), len(losses))\n",
    "\n",
    "        # Plot losses and show figure\n",
    "        plt.plot(x_axis,losses, 'r')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.autoscale(True,axis='y')\n",
    "        plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Class\n",
    "class LogReg:\n",
    "\n",
    "    # Initialize class object\n",
    "    def __init__(self, train_file='', test_file=''):\n",
    "\n",
    "        # QOL Checks for using default datasets\n",
    "        if train_file and not test_file:\n",
    "            # Read inputs and labels from datasets\n",
    "            self.train_inputs, self.train_labels = Parser.read_data(train_file)\n",
    "            warnings.warn(\"\\nWARNING in LogReg.__init__(): train_file given without test_file!\")\n",
    "\n",
    "        elif test_file and train_file:\n",
    "            # Read inputs and labels from datasets\n",
    "            self.train_inputs, self.train_labels = Parser.read_data(train_file)\n",
    "            self.test_inputs, self.test_labels = Parser.read_data(test_file)\n",
    "\n",
    "        else:\n",
    "            print(\"\\nERROR in LogReg.__init__(): No data given!\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        # Normalize input data\n",
    "        self.train_inputs = self.normalize(self.train_inputs)\n",
    "\n",
    "        # Read header data\n",
    "        self.headers = headers[0].split(',')[0:-1]\n",
    "\n",
    "        # Prepare / preprocess data\n",
    "        self.n_train_samples = 2600\n",
    "        self.prepare_data()\n",
    "\n",
    "        # Inputs that failed training\n",
    "        self.failures = []\n",
    "\n",
    "        # Model parameters\n",
    "        np.random.seed()\n",
    "        self.weights = np.random.normal(0,1,self.train_inputs.shape[1])\n",
    "        self.bias = np.random.normal(0,1,1)[0]\n",
    "\n",
    "        # Model data\n",
    "        self.errors = []            # List for keeping track of training progress\n",
    "        self.curr_loss = 0.0            # Current loss\n",
    "        self.trained = False\n",
    "\n",
    "    # Preprocess normalized input data\n",
    "    def prepare_data(self):\n",
    "\n",
    "        # Create an array of values from 0-len(train_inputs_norm)\n",
    "        indices = np.arange(self.train_inputs.shape[0])\n",
    "\n",
    "        # Randomly shuffle array of indices\n",
    "        indices = np.random.permutation(indices)\n",
    "\n",
    "        # Create validation sets from last 2600 randomly-aranged, normalized inputs\n",
    "        self.validation_inputs = self.train_inputs[indices[self.n_train_samples:]]\n",
    "        self.validation_labels = self.train_labels[indices[self.n_train_samples:]]\n",
    "\n",
    "        # Set training data as the rest of the normalized inputs\n",
    "        self.train_inputs = self.train_inputs[indices[:self.n_train_samples]]\n",
    "        self.train_labels = self.train_labels[indices[:self.n_train_samples]]\n",
    "\n",
    "\n",
    "    # Logistic (sigmoid) function\n",
    "    def sigmoid(self, x):\n",
    "        # New array with the specified precision\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Regularize weights\n",
    "    def L2(self):\n",
    "        return np.linalg.norm(self.weights)\n",
    "\n",
    "    # Normalize data\n",
    "    def normalize(self, data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "\n",
    "        return (data - mean) / std\n",
    "\n",
    "    # Compute weighted sum of inputs\n",
    "    def wsum(self, inputs):\n",
    "        return np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "    # Compute weighted sum and activate (Feed forward)\n",
    "    def feed_forward(self, inputs):\n",
    "        return self.sigmoid(self.wsum(inputs))\n",
    "\n",
    "    # Compute the cross entropy loss of a single prediction\n",
    "    # boost = lambda = regularization hyperparameter\n",
    "    def cross_entropy(self, predicted, actual, boost):\n",
    "        \n",
    "        if actual:\n",
    "            return -np.mean(np.log(predicted))\n",
    "        else:\n",
    "            return -np.mean(np.log((1.0 - predicted) + (boost/2.0)*self.L2()))\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss function\n",
    "    \"\"\"\n",
    "    def gradient(self, inputs, label, boost):\n",
    "        return np.dot(inputs, (self.feed_forward(inputs) - label)) + (boost/2)*self.L2()\n",
    "\n",
    "    # Make a prediction (either 0 or 1)\n",
    "    def predict(self, inputs):\n",
    "        return np.where(self.feed_forward(inputs) > 0.5, 1, 0)\n",
    "\n",
    "    # Perform a weight update\n",
    "    def update(self, inputs, label, learning_rate, boost):\n",
    "        # Compute weight gradient\n",
    "        gradient = self.gradient(inputs, label, boost)\n",
    "\n",
    "        # Update weight vector by stepping in the direction of neg. gradient\n",
    "        self.weights -= learning_rate * gradient\n",
    "\n",
    "        # Update bias\n",
    "        self.bias -=  learning_rate * (self.feed_forward(inputs) - label)\n",
    "\n",
    "\n",
    "    # Train model on data\n",
    "    def train(self, max_itrs=100, learning_rate=0.0001, boost=0.00001, stop=0.01, batch=''):\n",
    "        \n",
    "        # Initialize training results\n",
    "        train_results = {\n",
    "            'loss': [],\n",
    "            'iterations': 0\n",
    "        }\n",
    "\n",
    "        # Start and stop points for training\n",
    "        start = 0\n",
    "        end = len(self.train_inputs)\n",
    "\n",
    "        # Check if a batch has been specified\n",
    "        if len(batch) == 2:\n",
    "            start = int(batch[0])\n",
    "            end = int(batch[1])\n",
    "\n",
    "        # Loop through dataset\n",
    "        for i in range(start, end):\n",
    "            # Get input-label data\n",
    "            inputs = self.train_inputs[i]\n",
    "            label = self.train_labels[i]\n",
    "\n",
    "            # Compute initial output and loss\n",
    "            output = self.feed_forward(inputs)\n",
    "            loss = self.cross_entropy(output, label, boost)\n",
    "\n",
    "            # Current training iteration for this input\n",
    "            curr_itr = 0\n",
    "\n",
    "            # Loss values for the current input\n",
    "            input_losses = []\n",
    "            input_losses.append(loss)\n",
    "\n",
    "            # Add total loss at iteration to training results\n",
    "\n",
    "            # Train model on this input until it reaches acceptable margins for loss\n",
    "            # or max iterations reached\n",
    "            while loss > stop and curr_itr < max_itrs:\n",
    "\n",
    "                # Update model parameters\n",
    "                self.update(inputs, label, learning_rate, boost)\n",
    "\n",
    "                # Recompute output and loss\n",
    "                output = self.feed_forward(inputs)\n",
    "                loss = self.cross_entropy(output, label, boost)\n",
    "\n",
    "                # Increment current training iterations\n",
    "                curr_itr += 1\n",
    "\n",
    "                # Add current loss at current iteration to list\n",
    "                input_losses.append(loss)\n",
    "\n",
    "                # Add total loss at iteration to training results\n",
    "\n",
    "\n",
    "            # Check if input failed training\n",
    "            if loss < 0:\n",
    "                self.failures.append(start + i)\n",
    "\n",
    "                \n",
    "\n",
    "            # Plot losses, skip if curr_itr <= 1\n",
    "            print(\"Loss(input=\" + str(i) + \"): \" + str(loss) + \"\\titrs = \" + str(curr_itr))\n",
    "            \n",
    "\n",
    "            # Update training results\n",
    "            train_results['iterations'] += curr_itr\n",
    "\n",
    "            if loss > 0:\n",
    "                self.errors.append(1.0 - self.accuracy(self.train_inputs, self.train_labels))\n",
    "\n",
    "\n",
    "        \n",
    "        # Plot learning progress\n",
    "        plt_title = 'Training progress' + \"\\nMax Iterations = \" + str(max_itrs) + \"\\nLearning Rate = \" + str(learning_rate) + \"  Boost = \" + str(boost) + \"\\nCurrent Accuracy = \" + str(self.accuracy() * 100.0) + \"%\"\n",
    "        ModelVisualizer.plot_loss(self.errors, xlabel='Training iteration', ylabel='Loss (Error)', title=plt_title)\n",
    "        plt.pause(2)\n",
    "\n",
    "        \n",
    "        print(\"\\nMODEL TRAINED  (with %d failures)!\\n\" % (len(self.failures)))\n",
    "        self.trained = True\n",
    "        return train_results\n",
    "\n",
    "    # Compute accuracy of model of dataset\n",
    "    def accuracy(self, inputs, labels):\n",
    "        return np.sum((self.feed_forward(inputs)>0.5).astype(np.float64) == labels)  / labels.shape[0]\n",
    "\n",
    "\n",
    "    # Save model to file\n",
    "    def saveModel(self, filename):\n",
    "        file = open(filename, 'w')\n",
    "\n",
    "        # Store model parameters in file\n",
    "        file.write(\"Weights: \" + str(self.weights).rstrip(\"\\n\") + \"\\n\\n\")\n",
    "        file.write(\"Bias: \" + str(self.bias))\n",
    "\n",
    "        file.close()\n",
    "\n",
    "        # Check if save was successful\n",
    "        if os.path.exists(filename):\n",
    "            print(\"\\n\\nMODEL SAVED!\\nModel file = \" + str(filename))\n",
    "        else:\n",
    "            print(\"\\n\\nERROR in LogReg.saveModel(): Failed to save model!\")\n",
    "\n",
    "\n",
    "    # Load model from file\n",
    "    def loadModel(self, filename):\n",
    "\n",
    "        # Check if model exists\n",
    "        if os.path.exists(filename):\n",
    "            # Read lines from file\n",
    "            file = open(filename, 'r')\n",
    "            lines = file.readlines()\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"\\nERROR in LogReg.loadModel(): Model file not found!\")\n",
    "            return -1\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        # Remove label from weights' line\n",
    "        lines[0] = lines[0].replace('Weights: [', '')\n",
    "        lines[0] = lines[0].replace(']', '')\n",
    "\n",
    "        # Remove label from bias line\n",
    "        lines[-1] = lines[-1].replace('Bias: ', '')\n",
    "\n",
    "        # Extract bias\n",
    "        bias = float(lines[-1])\n",
    "\n",
    "        # Loop through weight lines\n",
    "        for i in range(len(lines)-1):\n",
    "            # Clean line\n",
    "            line_weights = lines[i].strip().split(',')\n",
    "            \n",
    "            while '' in line_weights:\n",
    "                line_weights.remove('')\n",
    "            \n",
    "            # Extract weights from line and append to list\n",
    "            for weight in line_weights:\n",
    "                weights.append(float(weight))\n",
    "\n",
    "        \n",
    "        # Set model params\n",
    "        self.weights = weights\n",
    "        self.bias = bias  \n",
    "\n",
    "        print(\"\\nMODEL LOADED SUCCESSFULLY!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test model on batch of data\n",
    "def runBatch(b_start, b_end, itrs, learn_rate, boost, logReg, train=True):\n",
    "    if train:   \n",
    "        # Compute accuracy before training\n",
    "        print(\"\\nModel accuracy before training: %.3f%%\\n\" % (logReg.accuracy()*100.0))\n",
    "\n",
    "        print(\"Training on batch [%d:%d] (size=%d)...\\n\" % (b_start, b_end, (b_start + b_end)))\n",
    "\n",
    "        # Train model on batch data\n",
    "        logReg.train(itrs, learn_rate, boost, batch=[b_start, b_end])\n",
    "\n",
    "        # Compute accuracy after training\n",
    "        print(\"\\nModel accuracy after training: %.3f%%\" % (logReg.accuracy()*100.0))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Train the model\n",
    "def train_model(lr):\n",
    "     \n",
    "    # Hyperparameters\n",
    "    max_train_itrs = 20000\n",
    "    learning_rate = 0.001\n",
    "    boost = 0.000001\n",
    "    min_lr = 1e-6       # Minimum learning rate\n",
    "    min_boost = 1e-9    # Minimum boost\n",
    "    mod_rate = 2        # Number of batches to train before updating hyperparams\n",
    "\n",
    "    \n",
    "\n",
    "    # Batch data\n",
    "    n_batches = 26 # 100 inputs per batch\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = (len(lr.train_inputs) / n_batches) * i              # Start index of batch\n",
    "        end = ((len(lr.train_inputs) / n_batches) * (i + 1)) - 1    # End index of batch\n",
    "\n",
    "        # Adjust boost and learning rate every other batch as model trains (to avoid overfitting)\n",
    "        if i % mod_rate == 0 and i > 0:\n",
    "            if learning_rate > min_lr:\n",
    "                # Last few updates\n",
    "                if learning_rate <= min_lr*10:\n",
    "                    max_train_itrs *= 0.7   # Update max training iterations\n",
    "                \n",
    "                mod_rate *= 2           # Change update rate\n",
    "                learning_rate /= i  # update learning rate\n",
    "\n",
    "            if boost > min_boost:\n",
    "                boost /= i\n",
    "\n",
    "        # Train model on batch\n",
    "        runBatch(start, end, max_train_itrs, learning_rate, boost, lr)\n",
    "    \n",
    "\n",
    "    plt_title = 'Training progress' + \"\\nMax Iterations = \" + str(max_train_itrs) + \"\\nLearning Rate = \" + str(learning_rate) + \"  Boost = \" + str(boost) + \"\\nCurrent Accuracy = \" + str(lr.accuracy() * 100.0) + \"%\"\n",
    "    ModelVisualizer.plot_loss(lr.errors, xlabel='Training iteration', ylabel='Loss (Error)', title=plt_title)\n",
    "\n",
    "    # Save model to file\n",
    "    lr.saveModel('model.obj')\n",
    "    #plt.savefig('model_progress.png')\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Test model\n",
    "def test_model(lr):\n",
    "    baseline_acc = 0.936\n",
    "\n",
    "    print(\"\\n\\nModel validation accuracy: %.2f%%\" % (lr.accuracy(lr.validation_inputs, lr.validation_labels) * 100.0))\n",
    "    print(\"Model test accuracy: %.2f%%\" % (lr.accuracy(lr.test_inputs, lr.test_labels) * 100.0))\n",
    "    print(\"Model is %.2f%% below the baseline\\n\" % ((baseline_acc - lr.accuracy(lr.test_inputs, lr.test_labels)) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL LOADED SUCCESSFULLY!\n",
      "\n",
      "\n",
      "\n",
      "Model validation accuracy: 93.91%\n",
      "Model test accuracy: 90.70%\n",
      "Model is 2.90% below the baseline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "   # Data files\n",
    "    trainfile = 'datasets/spambase-train.csv'\n",
    "    testfile = 'datasets/spambase-test.csv'\n",
    "\n",
    "    # Create LogisticRegression object\n",
    "    lr = LogReg(trainfile, testfile)\n",
    "\n",
    "    # train_model(lr)\n",
    "\n",
    "    # Load model from file\n",
    "    lr.loadModel('model.obj')\n",
    "\n",
    "    \n",
    "    # Test trained model\n",
    "    test_model(lr)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
