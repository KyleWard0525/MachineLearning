{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression using Binary Cross Entropy Loss with Gradient Descent\r\n",
    "\r\n",
    "### Kyle Ward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Global variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import sys\r\n",
    "import os\r\n",
    "import re\r\n",
    "import warnings\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# Add parent directory to system path to import mlutils\r\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\r\n",
    "import mlutils\r\n",
    "\r\n",
    "# File headers\r\n",
    "headers = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Classes\r\n",
    "#### *I realize the following two helper class should've just been global methods but its too late now"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parser for raw datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class Parser:\r\n",
    "\r\n",
    "    # Read in dataset (code from logistic-regression.ipynb)\r\n",
    "    @staticmethod\r\n",
    "    def read_data(filename):\r\n",
    "        global headers \r\n",
    "\r\n",
    "        f = open(filename, 'r')\r\n",
    "        p = re.compile(',')\r\n",
    "        xdata = []\r\n",
    "        ydata = []\r\n",
    "        header = f.readline().strip()\r\n",
    "        headers.append(header)\r\n",
    "        varnames = p.split(header)\r\n",
    "        namehash = {}\r\n",
    "        for l in f:\r\n",
    "            li = p.split(l.strip())\r\n",
    "            xdata.append([float(x) for x in li[:-1]])\r\n",
    "            ydata.append(float(li[-1]))\r\n",
    "        \r\n",
    "        return np.array(xdata), np.array(ydata)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model visualizer for plotting training progress"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Model Visualizer class\r\n",
    "class ModelVisualizer:\r\n",
    "\r\n",
    "    # Plot losses over iterations\r\n",
    "    @staticmethod\r\n",
    "    def plot_loss(losses, xlabel='', ylabel='', title=''):\r\n",
    "        plt.close()\r\n",
    "        plt.figure(figsize=(7,8))\r\n",
    "\r\n",
    "        # Create a linear space to plot\r\n",
    "        x_axis = np.linspace(0,len(losses), len(losses))\r\n",
    "\r\n",
    "        # Plot losses and show figure\r\n",
    "        plt.plot(x_axis,losses, 'r')\r\n",
    "        plt.xlabel(xlabel)\r\n",
    "        plt.ylabel(ylabel)\r\n",
    "        plt.title(title)\r\n",
    "        plt.autoscale(True,axis='y')\r\n",
    "        plt.show(block=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Logistic Regression Class\r\n",
    "class LogReg:\r\n",
    "\r\n",
    "    # Initialize class object\r\n",
    "    def __init__(self, train_file='', test_file=''):\r\n",
    "\r\n",
    "        # QOL Checks for using default datasets\r\n",
    "        if train_file and not test_file:\r\n",
    "            # Read inputs and labels from datasets\r\n",
    "            self.train_inputs, self.train_labels = Parser.read_data(train_file)\r\n",
    "            warnings.warn(\"\\nWARNING in LogReg.__init__(): train_file given without test_file!\")\r\n",
    "\r\n",
    "        elif test_file and train_file:\r\n",
    "            # Read inputs and labels from datasets\r\n",
    "            self.train_inputs, self.train_labels = Parser.read_data(train_file)\r\n",
    "            self.test_inputs, self.test_labels = Parser.read_data(test_file)\r\n",
    "\r\n",
    "        else:\r\n",
    "            print(\"\\nERROR in LogReg.__init__(): No data given!\")\r\n",
    "            return\r\n",
    "        \r\n",
    "\r\n",
    "        # Normalize input data\r\n",
    "        self.train_inputs = self.normalize(self.train_inputs)\r\n",
    "\r\n",
    "        # Read header data\r\n",
    "        self.headers = headers[0].split(',')[0:-1]\r\n",
    "\r\n",
    "        # Prepare / preprocess data\r\n",
    "        self.n_train_samples = 2600\r\n",
    "        self.prepare_data()\r\n",
    "\r\n",
    "        # Inputs that failed training\r\n",
    "        self.failures = []\r\n",
    "\r\n",
    "        # Model parameters\r\n",
    "        np.random.seed()\r\n",
    "        self.weights = np.random.normal(0,1,self.train_inputs.shape[1])\r\n",
    "        self.bias = np.random.normal(0,1,1)[0]\r\n",
    "\r\n",
    "        # Model data\r\n",
    "        self.errors = []            # List for keeping track of training progress\r\n",
    "        self.curr_loss = 0.0            # Current loss\r\n",
    "        self.trained = False\r\n",
    "\r\n",
    "    # Preprocess normalized input data\r\n",
    "    def prepare_data(self):\r\n",
    "\r\n",
    "        # Create an array of values from 0-len(train_inputs_norm)\r\n",
    "        indices = np.arange(self.train_inputs.shape[0])\r\n",
    "\r\n",
    "        # Randomly shuffle array of indices\r\n",
    "        indices = np.random.permutation(indices)\r\n",
    "\r\n",
    "        # Create validation sets from last 2600 randomly-aranged, normalized inputs\r\n",
    "        self.validation_inputs = self.train_inputs[indices[self.n_train_samples:]]\r\n",
    "        self.validation_labels = self.train_labels[indices[self.n_train_samples:]]\r\n",
    "\r\n",
    "        # Set training data as the rest of the normalized inputs\r\n",
    "        self.train_inputs = self.train_inputs[indices[:self.n_train_samples]]\r\n",
    "        self.train_labels = self.train_labels[indices[:self.n_train_samples]]\r\n",
    "\r\n",
    "\r\n",
    "    # Logistic (sigmoid) function\r\n",
    "    def sigmoid(self, x):\r\n",
    "        # New array with the specified precision\r\n",
    "        return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "    # Regularize weights\r\n",
    "    def L2(self):\r\n",
    "        return np.linalg.norm(self.weights)\r\n",
    "\r\n",
    "    # Normalize data\r\n",
    "    def normalize(self, data):\r\n",
    "        mean = np.mean(data)\r\n",
    "        std = np.std(data)\r\n",
    "\r\n",
    "        return (data - mean) / std\r\n",
    "\r\n",
    "    # Compute weighted sum of inputs\r\n",
    "    def wsum(self, inputs):\r\n",
    "        return np.dot(inputs, self.weights) + self.bias\r\n",
    "\r\n",
    "    # Compute weighted sum and activate (Feed forward)\r\n",
    "    def feed_forward(self, inputs):\r\n",
    "        return self.sigmoid(self.wsum(inputs))\r\n",
    "\r\n",
    "    # Compute the cross entropy loss of a single prediction\r\n",
    "    # boost = lambda = regularization hyperparameter\r\n",
    "    def cross_entropy(self, predicted, actual, boost):\r\n",
    "        \r\n",
    "        if actual:\r\n",
    "            return -np.mean(np.log(predicted))\r\n",
    "        else:\r\n",
    "            return -np.mean(np.log((1.0 - predicted) + (boost/2.0)*self.L2()))\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Compute the gradient of the cross entropy loss function\r\n",
    "    \"\"\"\r\n",
    "    def gradient(self, inputs, label, boost):\r\n",
    "        return np.dot(inputs, (self.feed_forward(inputs) - label)) + (boost/2)*self.L2()\r\n",
    "\r\n",
    "    # Make a prediction (either 0 or 1)\r\n",
    "    def predict(self, inputs):\r\n",
    "        return np.where(self.feed_forward(inputs) > 0.5, 1, 0)\r\n",
    "\r\n",
    "    # Perform a weight update\r\n",
    "    def update(self, inputs, label, learning_rate, boost):\r\n",
    "        # Compute weight gradient\r\n",
    "        gradient = self.gradient(inputs, label, boost)\r\n",
    "\r\n",
    "        # Update weight vector by stepping in the direction of neg. gradient\r\n",
    "        self.weights -= learning_rate * gradient\r\n",
    "\r\n",
    "        # Update bias\r\n",
    "        self.bias -=  learning_rate * (self.feed_forward(inputs) - label)\r\n",
    "\r\n",
    "\r\n",
    "    # Train model on data\r\n",
    "    def train(self, max_itrs=100, learning_rate=0.0001, boost=0.00001, stop=0.01, batch=''):\r\n",
    "        \r\n",
    "        # Initialize training results\r\n",
    "        train_results = {\r\n",
    "            'loss': [],\r\n",
    "            'iterations': 0\r\n",
    "        }\r\n",
    "\r\n",
    "        # Start and stop points for training\r\n",
    "        start = 0\r\n",
    "        end = len(self.train_inputs)\r\n",
    "\r\n",
    "        # Check if a batch has been specified\r\n",
    "        if len(batch) == 2:\r\n",
    "            start = int(batch[0])\r\n",
    "            end = int(batch[1])\r\n",
    "\r\n",
    "        # Loop through dataset\r\n",
    "        for i in range(start, end):\r\n",
    "            # Get input-label data\r\n",
    "            inputs = self.train_inputs[i]\r\n",
    "            label = self.train_labels[i]\r\n",
    "\r\n",
    "            # Compute initial output and loss\r\n",
    "            output = self.feed_forward(inputs)\r\n",
    "            loss = self.cross_entropy(output, label, boost)\r\n",
    "\r\n",
    "            # Current training iteration for this input\r\n",
    "            curr_itr = 0\r\n",
    "\r\n",
    "            # Loss values for the current input\r\n",
    "            input_losses = []\r\n",
    "            input_losses.append(loss)\r\n",
    "\r\n",
    "            # Add total loss at iteration to training results\r\n",
    "\r\n",
    "            # Train model on this input until it reaches acceptable margins for loss\r\n",
    "            # or max iterations reached\r\n",
    "            while loss > stop and curr_itr < max_itrs:\r\n",
    "\r\n",
    "                # Update model parameters\r\n",
    "                self.update(inputs, label, learning_rate, boost)\r\n",
    "\r\n",
    "                # Recompute output and loss\r\n",
    "                output = self.feed_forward(inputs)\r\n",
    "                loss = self.cross_entropy(output, label, boost)\r\n",
    "\r\n",
    "                # Increment current training iterations\r\n",
    "                curr_itr += 1\r\n",
    "\r\n",
    "                # Add current loss at current iteration to list\r\n",
    "                input_losses.append(loss)\r\n",
    "\r\n",
    "                # Add total loss at iteration to training results\r\n",
    "\r\n",
    "\r\n",
    "            # Check if input failed training\r\n",
    "            if loss < 0:\r\n",
    "                self.failures.append(start + i)\r\n",
    "\r\n",
    "                \r\n",
    "\r\n",
    "            # Plot losses, skip if curr_itr <= 1\r\n",
    "            print(\"Loss(input=\" + str(i) + \"): \" + str(loss) + \"\\titrs = \" + str(curr_itr))\r\n",
    "            \r\n",
    "\r\n",
    "            # Update training results\r\n",
    "            train_results['iterations'] += curr_itr\r\n",
    "\r\n",
    "            if loss > 0:\r\n",
    "                self.errors.append(1.0 - self.accuracy(self.train_inputs, self.train_labels))\r\n",
    "\r\n",
    "\r\n",
    "        \r\n",
    "        # Plot learning progress\r\n",
    "        plt_title = 'Training progress' + \"\\nMax Iterations = \" + str(max_itrs) + \"\\nLearning Rate = \" + str(learning_rate) + \"  Boost = \" + str(boost) + \"\\nCurrent Accuracy = \" + str(self.accuracy() * 100.0) + \"%\"\r\n",
    "        ModelVisualizer.plot_loss(self.errors, xlabel='Training iteration', ylabel='Loss (Error)', title=plt_title)\r\n",
    "        plt.pause(2)\r\n",
    "\r\n",
    "        \r\n",
    "        print(\"\\nMODEL TRAINED  (with %d failures)!\\n\" % (len(self.failures)))\r\n",
    "        self.trained = True\r\n",
    "        return train_results\r\n",
    "\r\n",
    "    # Compute accuracy of model of dataset\r\n",
    "    def accuracy(self, inputs, labels):\r\n",
    "        return np.sum((self.feed_forward(inputs)>0.5).astype(np.float64) == labels)  / labels.shape[0]\r\n",
    "\r\n",
    "\r\n",
    "    # Save model to file\r\n",
    "    def saveModel(self, filename):\r\n",
    "        file = open(filename, 'w')\r\n",
    "\r\n",
    "        # Store model parameters in file\r\n",
    "        file.write(\"Weights: \" + str(self.weights).rstrip(\"\\n\") + \"\\n\\n\")\r\n",
    "        file.write(\"Bias: \" + str(self.bias))\r\n",
    "\r\n",
    "        file.close()\r\n",
    "\r\n",
    "        # Check if save was successful\r\n",
    "        if os.path.exists(filename):\r\n",
    "            print(\"\\n\\nMODEL SAVED!\\nModel file = \" + str(filename))\r\n",
    "        else:\r\n",
    "            print(\"\\n\\nERROR in LogReg.saveModel(): Failed to save model!\")\r\n",
    "\r\n",
    "\r\n",
    "    # Load model from file\r\n",
    "    def loadModel(self, filename):\r\n",
    "\r\n",
    "        # Check if model exists\r\n",
    "        if os.path.exists(filename):\r\n",
    "            # Read lines from file\r\n",
    "            file = open(filename, 'r')\r\n",
    "            lines = file.readlines()\r\n",
    "            file.close()\r\n",
    "        else:\r\n",
    "            print(\"\\nERROR in LogReg.loadModel(): Model file not found!\")\r\n",
    "            return -1\r\n",
    "\r\n",
    "        weights = []\r\n",
    "\r\n",
    "        # Remove label from weights' line\r\n",
    "        lines[0] = lines[0].replace('Weights: [', '')\r\n",
    "        lines[0] = lines[0].replace(']', '')\r\n",
    "\r\n",
    "        # Remove label from bias line\r\n",
    "        lines[-1] = lines[-1].replace('Bias: ', '')\r\n",
    "\r\n",
    "        # Extract bias\r\n",
    "        bias = float(lines[-1])\r\n",
    "\r\n",
    "        # Loop through weight lines\r\n",
    "        for i in range(len(lines)-1):\r\n",
    "            # Clean line\r\n",
    "            line_weights = lines[i].strip().split(',')\r\n",
    "            \r\n",
    "            while '' in line_weights:\r\n",
    "                line_weights.remove('')\r\n",
    "            \r\n",
    "            # Extract weights from line and append to list\r\n",
    "            for weight in line_weights:\r\n",
    "                weights.append(float(weight))\r\n",
    "\r\n",
    "        \r\n",
    "        # Set model params\r\n",
    "        self.weights = weights\r\n",
    "        self.bias = bias  \r\n",
    "\r\n",
    "        print(\"\\nMODEL LOADED SUCCESSFULLY!\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training / Testing functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Train/Test model on batch of data\r\n",
    "def runBatch(b_start, b_end, itrs, learn_rate, boost, logReg, train=True):\r\n",
    "    if train:   \r\n",
    "        # Compute accuracy before training\r\n",
    "        print(\"\\nModel accuracy before training: %.3f%%\\n\" % (logReg.accuracy()*100.0))\r\n",
    "\r\n",
    "        print(\"Training on batch [%d:%d] (size=%d)...\\n\" % (b_start, b_end, (b_start + b_end)))\r\n",
    "\r\n",
    "        # Train model on batch data\r\n",
    "        logReg.train(itrs, learn_rate, boost, batch=[b_start, b_end])\r\n",
    "\r\n",
    "        # Compute accuracy after training\r\n",
    "        print(\"\\nModel accuracy after training: %.3f%%\" % (logReg.accuracy()*100.0))\r\n",
    "\r\n",
    "    else:\r\n",
    "        raise NotImplementedError\r\n",
    "\r\n",
    "# Train the model\r\n",
    "def train_model(lr):\r\n",
    "     \r\n",
    "    # Hyperparameters\r\n",
    "    max_train_itrs = 20000\r\n",
    "    learning_rate = 0.001\r\n",
    "    boost = 0.000001\r\n",
    "    min_lr = 1e-6       # Minimum learning rate\r\n",
    "    min_boost = 1e-9    # Minimum boost\r\n",
    "    mod_rate = 2        # Number of batches to train before updating hyperparams\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "    # Batch data\r\n",
    "    n_batches = 26 # 100 inputs per batch\r\n",
    "\r\n",
    "    for i in range(n_batches):\r\n",
    "        start = (len(lr.train_inputs) / n_batches) * i              # Start index of batch\r\n",
    "        end = ((len(lr.train_inputs) / n_batches) * (i + 1)) - 1    # End index of batch\r\n",
    "\r\n",
    "        # Adjust boost and learning rate every other batch as model trains (to avoid overfitting)\r\n",
    "        if i % mod_rate == 0 and i > 0:\r\n",
    "            if learning_rate > min_lr:\r\n",
    "                # Last few updates\r\n",
    "                if learning_rate <= min_lr*10:\r\n",
    "                    max_train_itrs *= 0.7   # Update max training iterations\r\n",
    "                \r\n",
    "                mod_rate *= 2           # Change update rate\r\n",
    "                learning_rate /= i  # update learning rate\r\n",
    "\r\n",
    "            if boost > min_boost:\r\n",
    "                boost /= i\r\n",
    "\r\n",
    "        # Train model on batch\r\n",
    "        runBatch(start, end, max_train_itrs, learning_rate, boost, lr)\r\n",
    "    \r\n",
    "\r\n",
    "    plt_title = 'Training progress' + \"\\nMax Iterations = \" + str(max_train_itrs) + \"\\nLearning Rate = \" + str(learning_rate) + \"  Boost = \" + str(boost) + \"\\nCurrent Accuracy = \" + str(lr.accuracy() * 100.0) + \"%\"\r\n",
    "    ModelVisualizer.plot_loss(lr.errors, xlabel='Training iteration', ylabel='Loss (Error)', title=plt_title)\r\n",
    "\r\n",
    "    # Save model to file\r\n",
    "    lr.saveModel('model.obj')\r\n",
    "    plt.savefig('model_progress.png')\r\n",
    "    \r\n",
    "    return lr\r\n",
    "\r\n",
    "# Test model\r\n",
    "def test_model(lr):\r\n",
    "    baseline_acc = 0.936\r\n",
    "\r\n",
    "    print(\"\\n\\nModel validation accuracy: %.2f%%\" % (lr.accuracy(lr.validation_inputs, lr.validation_labels) * 100.0))\r\n",
    "    print(\"Model test accuracy: %.2f%%\" % (lr.accuracy(lr.test_inputs, lr.test_labels) * 100.0))\r\n",
    "    print(\"Model is %.2f%% below the baseline\\n\" % ((baseline_acc - lr.accuracy(lr.test_inputs, lr.test_labels)) * 100.0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def main():\r\n",
    "   # Data files\r\n",
    "    trainfile = 'datasets/spambase-train.csv'\r\n",
    "    testfile = 'datasets/spambase-test.csv'\r\n",
    "\r\n",
    "    # Create LogisticRegression object\r\n",
    "    lr = LogReg(trainfile, testfile)\r\n",
    "\r\n",
    "    # train_model(lr)\r\n",
    "\r\n",
    "    # Load model from file\r\n",
    "    lr.loadModel('model.obj')\r\n",
    "\r\n",
    "    \r\n",
    "    # Test trained model\r\n",
    "    test_model(lr)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "MODEL LOADED SUCCESSFULLY!\n",
      "\n",
      "\n",
      "\n",
      "Model validation accuracy: 93.91%\n",
      "Model test accuracy: 90.70%\n",
      "Model is 2.90% below the baseline\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit"
  },
  "interpreter": {
   "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}