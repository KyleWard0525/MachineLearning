{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Assignment 1 - ID3\r\n",
    "## Kyle Ward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import re               #   Regular expression operations\r\n",
    "import math             #   Python standard math library\r\n",
    "import os               #   Operating system operations\r\n",
    "import time             #   Timing operations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Parser"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# For parsing and cleaning data\r\n",
    "class Parser:\r\n",
    "\r\n",
    "    # Initialize parser\r\n",
    "    def __init__(self, filename):\r\n",
    "        self.data = self.read_data(filename)    # Full dataset\r\n",
    "        self.vars = self.data[1]                # Attribute names (variables)\r\n",
    "        self.data = self.data[0]                # Train/Test data\r\n",
    "        self.get_labels()\r\n",
    "\r\n",
    "    # Load data from a file\r\n",
    "    @staticmethod\r\n",
    "    def read_data(filename):\r\n",
    "        f = open(filename, 'r')\r\n",
    "        p = re.compile(',')\r\n",
    "        data = []\r\n",
    "        header = f.readline().strip()\r\n",
    "        varnames = p.split(header)\r\n",
    "        namehash = {}\r\n",
    "        for l in f:\r\n",
    "            data.append([int(x) for x in p.split(l.strip())])\r\n",
    "        return (data, varnames)\r\n",
    "\r\n",
    "    # Get data labels\r\n",
    "    def get_labels(self):\r\n",
    "        self.data_labels = []\r\n",
    "\r\n",
    "        # Loop through all rows in data\r\n",
    "        for row in self.data:\r\n",
    "            # Add class label to list\r\n",
    "            self.data_labels.append(row[-1])\r\n",
    "\r\n",
    "    # Split dataset into data and labels\r\n",
    "    def split_data_labels(self, dataset):\r\n",
    "        labels = []\r\n",
    "        data = []\r\n",
    "\r\n",
    "        # Loop through rows in dataset\r\n",
    "        for row in dataset:\r\n",
    "            # Save label and remove from data row\r\n",
    "            labels.append(row[-1])\r\n",
    "            del(row[-1])\r\n",
    "\r\n",
    "            # Save new data row\r\n",
    "            data.append(row)\r\n",
    "\r\n",
    "        return [data, labels]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probability Helper Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Probability Helper Class\r\n",
    "class ID3_Probability:\r\n",
    "    # Find number of times an attribute is equal to a value\r\n",
    "    def count_occurrs(self, attr, val):\r\n",
    "        occurrs = 0\r\n",
    "\r\n",
    "        # Loop through dataset and count occurences\r\n",
    "        for row in self.data:\r\n",
    "            if row[attr] == val:\r\n",
    "                occurrs += 1\r\n",
    "\r\n",
    "        return occurrs\r\n",
    "\r\n",
    "    # Compute the probability distribution of a binary data vector\r\n",
    "    # Assuming class label is the last element\r\n",
    "    @staticmethod\r\n",
    "    def pdf(dataset):\r\n",
    "        occurrs = [0,0]\r\n",
    "        prob_dist = [0.0,0.0]\r\n",
    "\r\n",
    "        # Loop through dataset\r\n",
    "        for row in dataset:\r\n",
    "            if row[-1] == 0:\r\n",
    "                occurrs[0] += 1\r\n",
    "            elif row[-1] == 1:\r\n",
    "                occurrs[1] += 1\r\n",
    "\r\n",
    "        # Compute probability distribution \r\n",
    "        prob_dist = [occurrs[0] / len(dataset), occurrs[1] / len(dataset)]\r\n",
    "        \r\n",
    "        return prob_dist\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "    # Compute entropy of a probability distribution\r\n",
    "    @staticmethod\r\n",
    "    def entropy(prob_dist):\r\n",
    "        sum = 0.0\r\n",
    "\r\n",
    "        # Loop through probability distribution\r\n",
    "        for i in range(len(prob_dist)):\r\n",
    "            sum += prob_dist[i] * math.log2(prob_dist[i])\r\n",
    "\r\n",
    "        return -1.0 * sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ID3 Algorithm Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# ID3 Algorithm for Decision Trees\r\n",
    "class ID3:\r\n",
    "\r\n",
    "    # Initialize algorithm\r\n",
    "    def __init__(self, data_file=None):\r\n",
    "        \r\n",
    "        # Check if a data filename was passed in\r\n",
    "        if data_file:\r\n",
    "            # Create class object(s)\r\n",
    "            self.parser = Parser(data_file)\r\n",
    "\r\n",
    "            # Set class fields\r\n",
    "            self.data = self.parser.data\r\n",
    "            self.data_labels = self.parser.data_labels\r\n",
    "            self.attrNames = self.parser.vars\r\n",
    "            self.attributes = [*range(0,len(self.data[0])-1)]       # Fill array with values from 0 to the length of the data vectors\r\n",
    "\r\n",
    "            # Compute initial metrics\r\n",
    "            self.start_pdf = ID3_Probability.pdf(self.data)\r\n",
    "            self.start_entropy = ID3_Probability.entropy(self.start_pdf)       # NOTE: This variable will eventually represent previous_entropy\r\n",
    "\r\n",
    "        else:\r\n",
    "            raise RuntimeError(\"\\n\\nError in ID3.__init__(): \\n\\n'WHERE'S THE DATA?!?!?!?!?!????????'\\n\\n\\t-ID3 <3\\n\\n\")\r\n",
    "\r\n",
    "\r\n",
    "    # Split data based on a given attribute\r\n",
    "    def split(self, attr, data):\r\n",
    "\r\n",
    "        left = []                   # Left partition of the split\r\n",
    "        right = []                  # Right partition of the split\r\n",
    "\r\n",
    "        # Loop through rows in dataset\r\n",
    "        for row in data:\r\n",
    "\r\n",
    "            # Compare value of attribute in row with split value\r\n",
    "            if row[attr] < 1:\r\n",
    "                left.append(row)    # Add row to left partition\r\n",
    "            else:\r\n",
    "                right.append(row)   # Add row to right partition\r\n",
    "\r\n",
    "        # Create split\r\n",
    "        split = [left, right]\r\n",
    "\r\n",
    "        # If a split results in an empty branch, compute split metrics only on the non-empty branch\r\n",
    "        # Left branch is empty\r\n",
    "        if not left:\r\n",
    "            left_pdf = 0.0\r\n",
    "            left_entropy = 0.0\r\n",
    "\r\n",
    "            # Compute the class probability ditribution of the right side\r\n",
    "            right_pdf = ID3_Probability.pdf(right)\r\n",
    "\r\n",
    "            # Check for and remove zeros\r\n",
    "            if 0.0 in right_pdf:\r\n",
    "                right_pdf.remove(0.0)\r\n",
    "\r\n",
    "            # Compute the entropy of the right side\r\n",
    "            right_entropy = ID3_Probability.entropy(right_pdf)\r\n",
    "\r\n",
    "            # Compute the entropy of the entire split\r\n",
    "            weighted_avg_entropy = ((len(left) / len(data)) * left_entropy) + ((len(right) / len(data)) * right_entropy)\r\n",
    "\r\n",
    "            # Build split data\r\n",
    "            split = {\r\n",
    "            'left': left,\r\n",
    "            'right': right,\r\n",
    "            'left_entropy': left_entropy,\r\n",
    "            'right_entropy': right_entropy,\r\n",
    "            'wAvg_entropy': weighted_avg_entropy\r\n",
    "            }\r\n",
    "\r\n",
    "            # Return split\r\n",
    "            return split\r\n",
    "\r\n",
    "        # Right branch empty\r\n",
    "        elif not right:\r\n",
    "            right_pdf = 0.0\r\n",
    "            right_e = 0.0\r\n",
    "\r\n",
    "            # Compute the class probability ditribution of the left side\r\n",
    "            left_pdf = ID3_Probability.pdf(left)\r\n",
    "\r\n",
    "            # Check for and remove zeros\r\n",
    "            if 0.0 in left_pdf:\r\n",
    "                left_pdf.remove(0.0)\r\n",
    "\r\n",
    "            # Compute the entropy of the left side\r\n",
    "            left_e = ID3_Probability.entropy(left_pdf)\r\n",
    "\r\n",
    "            # Compute the entropy of the entire split\r\n",
    "            weighted_avg_entropy = ((len(left) / len(data)) * left_e) + ((len(right) / len(data)) * right_e)\r\n",
    "\r\n",
    "            split = {\r\n",
    "            'left': left,\r\n",
    "            'right': right,\r\n",
    "            'left_entropy': left_e,\r\n",
    "            'right_entropy': right_e,\r\n",
    "            'wAvg_entropy': weighted_avg_entropy\r\n",
    "            }\r\n",
    "\r\n",
    "            return split\r\n",
    "\r\n",
    "        # Neither branch is empty\r\n",
    "        else:\r\n",
    "\r\n",
    "            # Compute probability distribution of classes on left and right branch\r\n",
    "            left_pdf = ID3_Probability.pdf(split[0])\r\n",
    "            right_pdf= ID3_Probability.pdf(split[1])\r\n",
    "\r\n",
    "            # Check for and remove zeros\r\n",
    "            if 0.0 in left_pdf:\r\n",
    "                left_pdf.remove(0.0)\r\n",
    "            if 0.0 in right_pdf:\r\n",
    "                right_pdf.remove(0.0)\r\n",
    "\r\n",
    "            # Compute the entropy of the left and right branch\r\n",
    "            left_e = ID3_Probability.entropy(left_pdf)\r\n",
    "            right_e = ID3_Probability.entropy(right_pdf)\r\n",
    "\r\n",
    "            # Compute the entropy of the entire split\r\n",
    "            weighted_avg_entropy = ((len(split[0]) / len(data)) * left_e) + ((len(split[1]) / len(data)) * right_e)\r\n",
    "\r\n",
    "            split = {\r\n",
    "                'left': left,\r\n",
    "                'right': right,\r\n",
    "                'left_entropy': left_e,\r\n",
    "                'right_entropy': right_e,\r\n",
    "                'wAvg_entropy': weighted_avg_entropy\r\n",
    "            }\r\n",
    "\r\n",
    "            return split\r\n",
    "\r\n",
    "    # Find the best split for the given set of data\r\n",
    "    def best_split(self, data):\r\n",
    "    \r\n",
    "        gains = []      # Information gain scores for each attribute split\r\n",
    "\r\n",
    "        # Loop through all attributes to find attribute with best IG for best split\r\n",
    "        for attr_val in self.attributes:\r\n",
    "            split = self.split(attr_val, data)                  # Split data\r\n",
    "            wae = split['wAvg_entropy']                         # Weighted average entropy of the split\r\n",
    "            info_gain = self.start_entropy - wae                # Compute info gain relative to the entropy of the parent node\r\n",
    "            gains.append(info_gain)                             # Add info gain to list\r\n",
    "\r\n",
    "        # Create best split\r\n",
    "        best = {\r\n",
    "            'attr': gains.index(max(gains)),                    # Best split attribute is index of element with highest IG\r\n",
    "            'split': self.split(gains.index(max(gains)),data)   # Split on best attribute\r\n",
    "        }\r\n",
    "\r\n",
    "        # Update baseline entropy to be the weighted average entropy of best split (parent node)\r\n",
    "        self.start_entropy = best['split']['wAvg_entropy']\r\n",
    "\r\n",
    "        # Return best split (new node)\r\n",
    "        return best\r\n",
    "\r\n",
    "    # Create a leaf node\r\n",
    "    def leaf_node(self, data):\r\n",
    "        label_counts = [0,0]\r\n",
    "\r\n",
    "        # Count number of occurrences of each class label in the data\r\n",
    "        for row in data:\r\n",
    "            if row[-1] == 0:\r\n",
    "                label_counts[0] += 1\r\n",
    "            elif row[-1] == 1:\r\n",
    "                label_counts[1] += 1\r\n",
    "\r\n",
    "        # Value of the leaf is the index of the highest element (i.e the frequentist class label)\r\n",
    "        value = label_counts.index(max(label_counts))\r\n",
    "\r\n",
    "        return value\r\n",
    "\r\n",
    "    # Recursively build child nodes of the tree from the root node\r\n",
    "    def create_nodes(self, parent_node, max_depth, min_part_size, curr_depth):\r\n",
    "\r\n",
    "        # Create left and right partitions (branches) off of parent node\r\n",
    "        left = parent_node['split']['left']\r\n",
    "        right = parent_node['split']['right']\r\n",
    "\r\n",
    "        # Remove paritions from parent node\r\n",
    "        del(parent_node['split'])\r\n",
    "\r\n",
    "        # Check if either partition is empty, if so, create a leaf node (BASE CASE)\r\n",
    "        if not left or not right:\r\n",
    "            parent_node['left'] = parent_node['right'] = self.leaf_node(left + right)\r\n",
    "            return\r\n",
    "\r\n",
    "        # Max tree depth reached. Create leaf nodes (BASE CASE)\r\n",
    "        if curr_depth >= max_depth:\r\n",
    "            parent_node['left'] = self.leaf_node(left)\r\n",
    "            parent_node['right'] = self.leaf_node(right)\r\n",
    "            return\r\n",
    "\r\n",
    "\r\n",
    "        # First, build the left side of the tree recursively\r\n",
    "\r\n",
    "        # Check if left partition can be split again\r\n",
    "        # Partition too small to split\r\n",
    "        if len(left) <= min_part_size:\r\n",
    "            # Create a leaf node from partition\r\n",
    "            parent_node['left'] = self.leaf_node(left)\r\n",
    "\r\n",
    "        # Partition can be split again\r\n",
    "        else:\r\n",
    "            # Get best split (next node)\r\n",
    "            parent_node['left'] = self.best_split(left)\r\n",
    "\r\n",
    "            # Recursively build the rest of the left side of the tree\r\n",
    "            self.create_nodes(parent_node['left'], max_depth, min_part_size, curr_depth + 1)\r\n",
    "\r\n",
    "        \r\n",
    "        # Next, build the right side of the tree\r\n",
    "        \r\n",
    "        # Partition can't be split again\r\n",
    "        if len(right) <= min_part_size:\r\n",
    "            # Create leaf node\r\n",
    "            parent_node['right'] = self.leaf_node(right)\r\n",
    "        \r\n",
    "        # Partition can be split again\r\n",
    "        else:\r\n",
    "            # Get best split\r\n",
    "            parent_node['right'] = self.best_split(right)\r\n",
    "\r\n",
    "            # Recursively build the rest of the right side of the tree\r\n",
    "            self.create_nodes(parent_node['right'], max_depth, min_part_size, curr_depth + 1)\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    # Build decision tree from training data\r\n",
    "    def build_tree(self, data, min_part_size, max_depth):\r\n",
    "        \r\n",
    "        # Create root node by getting the best initial split\r\n",
    "        self.root_node = self.best_split(data)\r\n",
    "\r\n",
    "        # Build the rest of the tree from the root node\r\n",
    "        self.create_nodes(self.root_node, max_depth, min_part_size, 1)\r\n",
    "\r\n",
    "        # Return tree\r\n",
    "        return self.root_node\r\n",
    "\r\n",
    "\r\n",
    "    # Make a prediction by running data through the tree\r\n",
    "    @staticmethod\r\n",
    "    def predict(data, curr_node):\r\n",
    "        \r\n",
    "        # Check which side of the tree to travel down based on root node's attribute\r\n",
    "        if data[curr_node['attr']] < 1:\r\n",
    "\r\n",
    "            # Traverse left side of the tree\r\n",
    "\r\n",
    "            # Check if left branch leads to a decision node or leaf node\r\n",
    "            # Decision node\r\n",
    "            if isinstance(curr_node['left'], dict):\r\n",
    "                # Recursively trace down the left side of the tree\r\n",
    "                return ID3.predict(data, curr_node['left'])\r\n",
    "            \r\n",
    "            # Leaf node\r\n",
    "            else:\r\n",
    "                # Return classification\r\n",
    "                return curr_node['left']\r\n",
    "        else:\r\n",
    "\r\n",
    "            # Traverse the right side of the tree\r\n",
    "\r\n",
    "            # Check if right branch leads to a decision node or leaf node\r\n",
    "            # Decision node\r\n",
    "            if isinstance(curr_node['right'], dict):\r\n",
    "                # Recursively trace down the left side of the tree\r\n",
    "                return ID3.predict(data, curr_node['right'])\r\n",
    "            \r\n",
    "            # Leaf node\r\n",
    "            else:\r\n",
    "                return curr_node['right']\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run a test set, compute accuracy, and return the results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Compute the accuracy of a tree\r\n",
    "def accuracy(tree, data):\r\n",
    "\r\n",
    "    n_samples = len(data)                           # Total number of data samples\r\n",
    "    n_correct = 0                                   # Number of correct classifications\r\n",
    "\r\n",
    "    results = []                                    # Test results\r\n",
    "\r\n",
    "    # Loop through each row (test) in the data set\r\n",
    "    for row in data:\r\n",
    "\r\n",
    "        # Make prediction and get actual value\r\n",
    "        prediction = ID3.predict(row, tree)\r\n",
    "        actual = row[-1]\r\n",
    "\r\n",
    "        # Check if prediction is correct\r\n",
    "        if prediction == actual:\r\n",
    "            n_correct += 1\r\n",
    "        \r\n",
    "        # Create results for this test row\r\n",
    "        test_res = {\r\n",
    "            'input': row,\r\n",
    "            'prediction': prediction,\r\n",
    "            'actual': actual\r\n",
    "        }\r\n",
    "\r\n",
    "        # Save to overall results\r\n",
    "        results.append(test_res)\r\n",
    "\r\n",
    "    # Compute accuracy and store in results\r\n",
    "    acc = float(n_correct / n_samples)\r\n",
    "    results.append({'accuracy': acc})\r\n",
    "\r\n",
    "    # Return test results\r\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a tree for each data set and run it's corresponding test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Build tree for each training set using ID3\r\n",
    "def runAllTests():\r\n",
    "    numSets = 3\r\n",
    "\r\n",
    "    # Data files\r\n",
    "    train_files = [\"data_sets1/training_set.csv\", \"data_sets2/training_set.csv\", \"agaricuslepiotatrain1.csv\"]\r\n",
    "    validation_files = [\"data_sets1/validation_set.csv\", \"data_sets2/validation_set.csv\"]\r\n",
    "    test_files = [\"data_sets1/test_set.csv\", \"data_sets2/test_set.csv\", \"agaricuslepiotatest1.csv\"]\r\n",
    "\r\n",
    "    # Data sets\r\n",
    "    train_sets = [Parser.read_data(train_files[0])[0], Parser.read_data(train_files[1])[0], Parser.read_data(train_files[2])[0]]\r\n",
    "    test_sets = [Parser.read_data(test_files[0])[0], Parser.read_data(test_files[1])[0], Parser.read_data(test_files[2])[0]]\r\n",
    "    valid_sets = [Parser.read_data(validation_files[0])[0], Parser.read_data(validation_files[1])[0]]\r\n",
    "\r\n",
    "    # Decision trees\r\n",
    "    trees = []\r\n",
    "\r\n",
    "    # Training parameters\r\n",
    "    min_partition_size = 1\r\n",
    "    max_depth = 15\r\n",
    "\r\n",
    "    for i in range(numSets):\r\n",
    "        # Create instance of ID3 algorithm with training data\r\n",
    "        id3 = ID3(train_files[i])\r\n",
    "\r\n",
    "        # Build tree and append to list of trees\r\n",
    "        tree = id3.build_tree(id3.data, min_partition_size, max_depth)\r\n",
    "\r\n",
    "        # Compute tree's accuracy on training, validation, and test sets\r\n",
    "        train_res = accuracy(tree, train_sets[i])\r\n",
    "        test_res = accuracy(tree, test_sets[i])\r\n",
    "\r\n",
    "        # Check if test had a validation set\r\n",
    "        if i < 2:\r\n",
    "            valid_res = accuracy(tree, valid_sets[i])\r\n",
    "        else:\r\n",
    "            valid_res = 'No validation file.'\r\n",
    "\r\n",
    "        # Create dict of tree and info\r\n",
    "        tree_info = {\r\n",
    "            'id': i,\r\n",
    "            'tree': tree,\r\n",
    "            'train_results': train_res,\r\n",
    "            'valid_results': valid_res,\r\n",
    "            'test_results': test_res\r\n",
    "        }\r\n",
    "        \r\n",
    "        # Save to list of trees\r\n",
    "        trees.append(tree_info)\r\n",
    "\r\n",
    "    # Return decision trees\r\n",
    "    return trees"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and test all decision tress then save the results to their respective files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Test decision trees against all test sets and save results to seperate files\r\n",
    "def test_and_save():\r\n",
    "\r\n",
    "    # Create and test all decision trees\r\n",
    "    trees = runAllTests()\r\n",
    "\r\n",
    "    # Check if results directory exists\r\n",
    "    if not os.path.exists(\"/results\"):\r\n",
    "        os.system(\"mkdir results\")\r\n",
    "\r\n",
    "    # Loop through all trees\r\n",
    "    for i in range(len(trees)):\r\n",
    "        # Open output file and get current decision tree\r\n",
    "        tree_file = open((\"results/tree_%d.txt\" % i), \"w\")\r\n",
    "        tree = trees[i]\r\n",
    "\r\n",
    "        # Write information about tree to the file\r\n",
    "        tree_file.write(\"\\nTREE ID: \" + str(tree['id']))\r\n",
    "        tree_file.write(\"\\n\\nTREE STRUCTURE: \\n\" + str(tree['tree']))\r\n",
    "        tree_file.write(\"\\n\\nTEST RESULTS: \\n\" + str(tree['test_results']))\r\n",
    "\r\n",
    "        # Write tree test accuracy to file\r\n",
    "        tree_file.write(\"\\n\\nTREE TEST ACCURACY: %.3f%%\" % (tree['test_results'][-1]['accuracy'] * 100.0))\r\n",
    "\r\n",
    "        # Check if trees has validation test results\r\n",
    "        if tree['valid_results'] != \"No validation file.\":\r\n",
    "            tree_file.write(\"\\n\\nTREE VALIDATION ACCURACY: %.3f%%\" % (tree['valid_results'][-1]['accuracy'] * 100.0))\r\n",
    "        else:\r\n",
    "            tree_file.write(\"\\n\\nVALIDATION RESULTS: \\n\" + str(tree['valid_results']))\r\n",
    "\r\n",
    "        # Close file\r\n",
    "        tree_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the project\r\n",
    "\r\n",
    "### Build a decision tree from each dataset, test the tree, and save the results to the results directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Main function\r\n",
    "def main():\r\n",
    "\r\n",
    "    print(\"\\n\\nBUILDING AND TESTING DECISION TREES...\\n\")\r\n",
    "\r\n",
    "    # For user-readability and dramatic effect\r\n",
    "    time.sleep(1.5)\r\n",
    "\r\n",
    "    # Build, test, and store results of decision trees over the 3 datasets\r\n",
    "    test_and_save()\r\n",
    "\r\n",
    "    print(\"TESTING COMPLETE!\\nRESULTS HAVE BEEN SAVED UNDER '/results'\\n\\n\")\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    try:\r\n",
    "        main()\r\n",
    "    except Exception:\r\n",
    "        print(\"\\n\\nAn unexpected error has occurred!\\n\\n\")\r\n",
    "        quit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "BUILDING AND TESTING DECISION TREES...\n",
      "\n",
      "TESTING COMPLETE!\n",
      "RESULTS HAVE BEEN SAVED UNDER '/results'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## &emsp; To see the test result files, see \"/results\" after running the project\r\n",
    "\r\n",
    "- ### NOTE: The test id's correspond to the data sets as follows:\r\n",
    "    -   0:  data_sets1\r\n",
    "    -   1:  data_sets2\r\n",
    "    -   2:  agaricuslepiotatest"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit"
  },
  "interpreter": {
   "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}